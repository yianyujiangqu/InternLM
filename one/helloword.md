# 书生·浦语 2.0
第一节笔记
## 书生·浦语大模型全链路开源体系
### 大模型发展成通用人工智能的重要途经：
- 专用模型：针对特定，单一
- 通用大模型：应对多种任务，多模态
### 书生·浦语 2.0（InternLM）：
#### **体系**
- 7B：为轻量级的研究和应用提供了一个轻便但性能不俗的模型。
- 20B：模型的综合性能更为强劲，可有效支持更加复杂的使用场景。
#### **三个版本**
- InternLM2-Base：高质量和具有很强可塑性的模型基座，是模型进行深度领域适配的高质量起点。
- InternLM2：在Base基础上，在多个能力方向进行了强化，在评测中成绩优异，同时保持了很好的通用语言能力
- InternLM2-Chat：在Base基础上，经过SFT和RLHF，面向对话交互进行了优化，具有很好的指令遵循、共情聊天和调用工具等的能力。
### 回归语言建模的本质
#### 采用新一代数据清洗过滤技术：
- 多维度数据价值评估
- 高质量语料驱动的数据富集
- 有针对性的数据补齐
### 书生·浦语 2.0（InternLM）主要亮点：
- 超长上下文：20万token上下文，几乎完美实现“大海捞针”
- 综合性能 全面提升
- 优秀的对话和创作体验
- 工具调用能力整体升级
- 突出的数理能力和实用的数据分析功能
### 书生·浦语全链条开源开放体系：
- 数据：书生·万卷，2TB数据，涵盖多种模态与任务
- 预训练：InternLM-Train，并行训练，极致优化，速度达到3600tokens/sec/gpu
- 微调：XTuner，支持全参数微调，支持LoRA等低成本微调
- 部署：LMDeploy，全链路部署，性能领先，每秒生成2000+tokens
- 评测：OpenCompass，全方位评测，性能可复现，100套评测套，50万道题
- 应用：Lagent AgentLego，支持多种智能体，支持代码解释器等多种工具
